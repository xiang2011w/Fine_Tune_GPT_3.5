# Medical AI Fine-tuning Project

## Overview

This project focuses on fine-tuning an OpenAI `gpt-3.5-turbo` model with a custom medical dataset (`medical_o1_sft_mix.json`). The primary objective is to create a specialized AI assistant capable of:
1.  Understanding medical questions.
2.  Generating a detailed chain-of-thought (CoT) to explain its reasoning.
3.  Providing a concise final response based on the CoT.

The fine-tuning process utilizes OpenAI's API and a structured dataset to tailor the model's responses to specific medical query patterns and output formats.

## Features

*   **Specialized Medical Knowledge:** Fine-tuned on the `medical_o1_sft_mix.json` dataset.
*   **Chain-of-Thought (CoT) Generation:** The model is explicitly trained to output a step-by-step reasoning process (`Complex_CoT`) before delivering the final answer.
*   **Concise Responses:** Alongside the detailed CoT, the model provides a direct and summarized answer (`Response`) to the input question.
*   **OpenAI API Integration:** Leverages the official OpenAI Python library for both the fine-tuning process and for making inferences with the fine-tuned model.

## Tech Stack

*   **Python 3.x** (Developed with 3.7+)
*   **OpenAI API**
    *   Base Model for Fine-tuning: `gpt-3.5-turbo-0125` (or the specific version used)
*   **Core Python Libraries:**
    *   `openai`: For interacting with the OpenAI API.
    *   `python-dotenv`: For managing environment variables (like API keys).
    *   `tiktoken` (Optional, for development): Used for token counting during data analysis and cost estimation.

## Project Structure

```text
.
├── .env                  # Stores API keys and other environment variables (DO NOT COMMIT)
├── .gitignore            # Specifies intentionally untracked files by Git
├── medical_o1_sft_mix.json # Original input data in JSON format
├── medical_train.jsonl   # Formatted training data for OpenAI fine-tuning
├── medical_valid.jsonl   # Formatted validation data for OpenAI fine-tuning (optional)
├── prepare_data.py       # Python script to convert input JSON to JSONL format
├── run_finetuning.py     # Python script to manage the fine-tuning job with OpenAI
├── fine_tuned_model_id.txt # Stores the ID of the successfully fine-tuned model (generated after run)
└── README.md             # This file: project documentation
```

## Setup and Installation

1.  **Prerequisites:**
    *   Python 3.7 or higher.
    *   `pip` (Python package installer).
    *   An active OpenAI API account with sufficient quota/billing enabled for fine-tuning.

2.  **Clone the Repository (if applicable):**
    ```bash
    git clone <your-repository-url>
    cd <repository-name>
    ```

3.  **Install Dependencies:**
    ```bash
    pip install openai python-dotenv tiktoken
    ```
    (Consider creating a `requirements.txt` file for easier dependency management).

4.  **Set Up Environment Variables:**
    Create a file named `.env` in the project's root directory. Add your OpenAI API key to this file:
    ```text:.env
    OPENAI_API_KEY="sk-your_actual_openai_api_key_here"
    ```
    Replace `"sk-your_actual_openai_api_key_here"` with your real OpenAI API key. The `.env` file is included in `.gitignore` to prevent accidental commitment of sensitive credentials.

## Data Preparation

The fine-tuning process requires data to be in a specific JSONL (JSON Lines) format.

1.  **Input Data Source:**
    The project expects the initial medical data in a JSON file named `medical_o1_sft_mix.json`. This file should contain an array of objects, where each object has at least the following keys: `Question`, `Complex_CoT`, and `Response`.

2.  **Conversion to JSONL:**
    The `prepare_data.py` script is used to transform `medical_o1_sft_mix.json` into the required JSONL format.
    ```bash
    python prepare_data.py
    ```
    This script will:
    *   Read the input JSON data.
    *   Format each data entry into the OpenAI messages structure, including a system prompt, the user's question, and the assistant's combined CoT and response.
    *   Split the data into a training set (`medical_train.jsonl`) and a validation set (`medical_valid.jsonl`).
    *   Output these files to the project root.

## Fine-tuning Process

The `run_finetuning.py` script automates the process of submitting and monitoring the fine-tuning job on OpenAI.

1.  **Configuration (Optional):**
    Review and modify constants at the top of `run_finetuning.py` if necessary (e.g., `BASE_MODEL`, `MODEL_SUFFIX`).

2.  **Execute the Fine-tuning Script:**
    Ensure your `.env` file is correctly set up and the `medical_train.jsonl` (and optionally `medical_valid.jsonl`) files have been generated by `prepare_data.py`.
    ```bash
    python run_finetuning.py
    ```
    This script performs the following actions:
    *   Loads the OpenAI API key from the `.env` file.
    *   Uploads the `medical_train.jsonl` and `medical_valid.jsonl` files to OpenAI.
    *   Initiates a new fine-tuning job with the specified base model and uploaded files.
    *   Continuously monitors the status of the fine-tuning job.
    *   Upon successful completion, it prints the ID of the newly fine-tuned model and saves this ID to `fine_tuned_model_id.txt`.

## Using the Fine-tuned Model

After a successful fine-tuning job, the model ID stored in `fine_tuned_model_id.txt` can be used to make API calls.

Example Python snippet for inference:
```python
from openai import OpenAI
import os
from dotenv import load_dotenv

# Load API key
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("OPENAI_API_KEY not found in .env file. Please create .env with your key.")

client = OpenAI(api_key=api_key)

# Load your fine-tuned model ID
FINE_TUNED_MODEL_ID = ""
try:
    with open("fine_tuned_model_id.txt", "r") as f:
        FINE_TUNED_MODEL_ID = f.read().strip()
except FileNotFoundError:
    print("Warning: fine_tuned_model_id.txt not found. Set FINE_TUNED_MODEL_ID manually if you have it.")
    # Example: FINE_TUNED_MODEL_ID = "ft:gpt-3.5-turbo-0125:your-org::idstring123" 

if not FINE_TUNED_MODEL_ID:
    print("Error: Fine-tuned model ID is not available.")
    exit()

# The system prompt should ideally match the one used during training for consistency
SYSTEM_PROMPT = (
    "You are an expert medical AI assistant. Given a question, first provide a "
    "detailed chain of thought (Complex_CoT) explaining your reasoning step-by-step. "
    "After the chain of thought, conclude with a concise final Response."
)

def get_medical_ai_response(question: str):
    if not FINE_TUNED_MODEL_ID:
        print("Fine-tuned model ID is not set.")
        return "Error: Model ID not available."
    try:
        completion = client.chat.completions.create(
            model=FINE_TUNED_MODEL_ID,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": question}
            ],
            temperature=0.5,  # Adjust for desired level of determinism/creativity
            max_tokens=1500   # Adjust based on expected length of CoT + Response
        )
        return completion.choices[0].message.content
    except Exception as e:
        return f"An error occurred: {e}"

if __name__ == "__main__":
    if FINE_TUNED_MODEL_ID:
        sample_question = "A 60-year-old patient reports sudden onset of chest pain radiating to the left arm. What are the immediate concerns and differential diagnoses?"
        print(f"Sending question to model {FINE_TUNED_MODEL_ID}: {sample_question}\n")
        response_content = get_medical_ai_response(sample_question)
        print("\n--- Model Response ---")
        print(response_content)
        print("--- End Model Response ---")
    else:
        print("Cannot run example: Fine-tuned model ID is missing.")
```

## Cost Considerations

*   **Fine-tuning Costs:** OpenAI charges for fine-tuning based on the total number of tokens processed (which is `number_of_tokens_in_training_file * number_of_epochs`). For `gpt-3.5-turbo`, the rate is $0.0080 per 1,000 tokens. Large datasets or many epochs can result in significant costs.
*   **Inference Costs:** Using your fine-tuned model for generating responses also incurs costs, priced per 1,000 input tokens and 1,000 output tokens. For fine-tuned `gpt-3.5-turbo` models, these rates are typically $0.0030/1K input tokens and $0.0060/1K output tokens (these rates are subject to change by OpenAI).
*   Always monitor your usage and costs via the OpenAI dashboard and set spending limits if necessary.

## Important Disclaimer

**This project and the resulting fine-tuned model are intended for research, development, and illustrative purposes ONLY. The medical information generated by this AI model should NEVER be used for actual medical diagnosis, treatment decisions, or any form of professional medical advice.**

The outputs of AI models, especially in sensitive domains like medicine, can be inaccurate, incomplete, or biased. Always consult with qualified healthcare professionals for any medical concerns or decisions. The developers of this project assume no liability for any actions taken based on the model's output.

## Future Improvements (Potential)

*   Expand and diversify the training dataset with more examples and edge cases.
*   Implement a more rigorous evaluation framework using a hold-out test set and defined metrics.
*   Experiment with different system prompts or fine-tuning parameters (e.g., learning rate, if applicable for future models).
*   Develop a user-friendly interface (e.g., a simple web app) for interacting with the fine-tuned model.
*   Explore techniques for continuous learning or model updates as new data becomes available.